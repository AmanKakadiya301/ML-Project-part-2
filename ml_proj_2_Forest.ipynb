{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XuyBgdCja5q",
        "outputId": "b033abd2-479b-4b7d-c830-ada26532a734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b7d128d"
      },
      "source": [
        "### 1. IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37a55505"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62f9c2b3"
      },
      "source": [
        "### 2. CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3721c3e"
      },
      "source": [
        "RANDOM_SEED   = 42\n",
        "BATCH_SIZE    = 2048        # smaller batch than 4096 → usually better generalization\n",
        "LR            = 0.001\n",
        "EPOCHS        = 120         # more than old 80, but with early stopping\n",
        "PATIENCE      = 12\n",
        "WARMUP_EPOCHS = 5\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "418db109"
      },
      "source": [
        "### 3. LOAD CSV FROM /content/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c79df610"
      },
      "source": [
        "csv_path = \"/content/covtype.csv\"\n",
        "assert os.path.exists(csv_path), \"covtype.csv NOT FOUND in /content. Upload it in Colab files.\"\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "print(\"Raw shape:\", df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eaf2f46"
      },
      "source": [
        "### 4. BASIC CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7cb99ed"
      },
      "source": [
        "# Drop rows with missing target\n",
        "df = df.dropna(subset=[\"Cover_Type\"])\n",
        "\n",
        "# Ensure integer target\n",
        "df[\"Cover_Type\"] = df[\"Cover_Type\"].astype(int)\n",
        "\n",
        "# Replace infinities and drop remaining NaNs (if any)\n",
        "df = df.replace([np.inf, -np.inf], np.nan)\n",
        "df = df.dropna()\n",
        "\n",
        "print(\"Cleaned shape:\", df.shape)\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb4ce183"
      },
      "source": [
        "### 5. FEATURE ENGINEERING (richer but not crazy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7666109"
      },
      "source": [
        "cont_cols = [\n",
        "    \"Elevation\", \"Aspect\", \"Slope\",\n",
        "    \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\",\n",
        "    \"Horizontal_Distance_To_Roadways\",\n",
        "    \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
        "    \"Horizontal_Distance_To_Fire_Points\",\n",
        "]\n",
        "\n",
        "# Core extra features\n",
        "df[\"mean_hillshade\"] = (\n",
        "    df[\"Hillshade_9am\"] + df[\"Hillshade_Noon\"] + df[\"Hillshade_3pm\"]\n",
        ") / 3.0\n",
        "\n",
        "df[\"hydro_mag\"] = np.sqrt(\n",
        "    df[\"Horizontal_Distance_To_Hydrology\"]**2 +\n",
        "    df[\"Vertical_Distance_To_Hydrology\"]**2\n",
        ")\n",
        "\n",
        "df[\"hydro_road\"] = df[\"Horizontal_Distance_To_Hydrology\"] / (\n",
        "    df[\"Horizontal_Distance_To_Roadways\"].abs() + 1.0\n",
        ")\n",
        "\n",
        "df[\"fire_road\"] = (\n",
        "    df[\"Horizontal_Distance_To_Fire_Points\"] -\n",
        "    df[\"Horizontal_Distance_To_Roadways\"]\n",
        ")\n",
        "\n",
        "df[\"elev_hydro_diff\"] = df[\"Elevation\"] - df[\"Vertical_Distance_To_Hydrology\"]\n",
        "df[\"elev_road_diff\"]  = df[\"Elevation\"] - df[\"Horizontal_Distance_To_Roadways\"]\n",
        "df[\"elev_fire_diff\"]  = df[\"Elevation\"] - df[\"Horizontal_Distance_To_Fire_Points\"]\n",
        "\n",
        "df[\"shade_diff\"] = df[\"Hillshade_Noon\"] - df[\"Hillshade_9am\"]\n",
        "df[\"sun_intensity\"] = (\n",
        "    df[\"Hillshade_9am\"] + df[\"Hillshade_Noon\"] + df[\"Hillshade_3pm\"]\n",
        ")\n",
        "\n",
        "df[\"slope_elev_ratio\"] = df[\"Slope\"] / (df[\"Elevation\"] + 1.0)\n",
        "df[\"aspect_sin\"] = np.sin(np.deg2rad(df[\"Aspect\"]))\n",
        "df[\"aspect_cos\"] = np.cos(np.deg2rad(df[\"Aspect\"]))\n",
        "\n",
        "df[\"terrain_index\"] = df[\"Elevation\"] + df[\"Vertical_Distance_To_Hydrology\"]\n",
        "df[\"moisture\"] = df[\"hydro_mag\"] / (df[\"Slope\"] + 1.0)\n",
        "df[\"sun_elev_ratio\"] = df[\"sun_intensity\"] / (df[\"Elevation\"] + 1.0)\n",
        "\n",
        "soil_cols = [c for c in df.columns if c.startswith(\"Soil_Type\")]\n",
        "df[\"soil_type_count\"] = df[soil_cols].sum(axis=1)\n",
        "\n",
        "extra_cols = [\n",
        "    \"mean_hillshade\", \"hydro_mag\", \"hydro_road\", \"fire_road\",\n",
        "    \"elev_hydro_diff\", \"elev_road_diff\", \"elev_fire_diff\",\n",
        "    \"shade_diff\", \"sun_intensity\",\n",
        "    \"slope_elev_ratio\", \"aspect_sin\", \"aspect_cos\",\n",
        "    \"terrain_index\", \"moisture\", \"sun_elev_ratio\", \"soil_type_count\",\n",
        "]\n",
        "\n",
        "all_cont_cols = cont_cols + extra_cols\n",
        "print(\"Total continuous columns:\", len(all_cont_cols))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b8e0f2b"
      },
      "source": [
        "### 6. SPLIT DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f6ca9c3"
      },
      "source": [
        "X = df.drop(\"Cover_Type\", axis=1)\n",
        "y = df[\"Cover_Type\"].values - 1   # labels: 0..6\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.30, stratify=y, random_state=RANDOM_SEED\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(\"TRAIN:\", X_train.shape, \"VAL:\", X_val.shape, \"TEST:\", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "547223e3"
      },
      "source": [
        "### 7. SCALE CONTINUOUS COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92b41242"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_s = X_train.copy()\n",
        "X_val_s   = X_val.copy()\n",
        "X_test_s  = X_test.copy()\n",
        "\n",
        "X_train_s[all_cont_cols] = scaler.fit_transform(X_train[all_cont_cols])\n",
        "X_val_s[all_cont_cols]   = scaler.transform(X_val[all_cont_cols])\n",
        "X_test_s[all_cont_cols]  = scaler.transform(X_test[all_cont_cols])\n",
        "\n",
        "X_train_np = X_train_s.values.astype(np.float32)\n",
        "X_val_np   = X_val_s.values.astype(np.float32)\n",
        "X_test_np  = X_test_s.values.astype(np.float32)\n",
        "\n",
        "y_train_np = y_train.astype(np.int64)\n",
        "y_val_np   = y_val.astype(np.int64)\n",
        "y_test_np  = y_test.astype(np.int64)\n",
        "\n",
        "input_dim   = X_train_np.shape[1]\n",
        "num_classes = len(np.unique(y_train_np))\n",
        "\n",
        "print(\"Input dim =\", input_dim, \"| Classes =\", num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ffd459"
      },
      "source": [
        "### 8. DATASET & DATALOADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78da7fc8"
      },
      "source": [
        "class CoverDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "train_loader = DataLoader(CoverDataset(X_train_np, y_train_np), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(CoverDataset(X_val_np, y_val_np), batch_size=BATCH_SIZE)\n",
        "test_loader  = DataLoader(CoverDataset(X_test_np, y_test_np), batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "534664c2"
      },
      "source": [
        "### 9. MODEL — STABLE DEEP MLP (from 95% version, slight tweak)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54ba975c"
      },
      "source": [
        "class DeepMLP(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        act = nn.LeakyReLU(0.01)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024), act,\n",
        "            nn.Linear(1024, 512), act,\n",
        "            nn.Linear(512, 512), act,\n",
        "            nn.Linear(512, 256), act,\n",
        "            nn.Linear(256, 256), act,\n",
        "            nn.Linear(256, 128), act,\n",
        "            nn.Linear(128, 64), act,\n",
        "            nn.Linear(64, num_classes),\n",
        "        )\n",
        "\n",
        "        # Xavier init\n",
        "        for m in self.net:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = DeepMLP(input_dim, num_classes).to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3560d037"
      },
      "source": [
        "### 10. LOSS + OPTIMIZER + SCHEDULER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96b1622a"
      },
      "source": [
        "class_weights = compute_class_weight(\n",
        "    \"balanced\", classes=np.unique(y_train_np), y=y_train_np\n",
        ")\n",
        "# Soften extremely strong weights a bit (sqrt) to avoid over-penalizing\n",
        "class_weights = np.sqrt(np.clip(class_weights, 0.2, 8.0))\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "\n",
        "# Smooth cosine schedule\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f99cf4"
      },
      "source": [
        "### 11. TRAIN LOOP (with warmup + early stopping)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c97012a"
      },
      "source": [
        "best_loss = np.inf\n",
        "best_state = None\n",
        "pat = 0\n",
        "\n",
        "train_losses, val_losses, val_accs = [], [], []\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    # Manual warmup for first WARMUP_EPOCHS\n",
        "    if epoch <= WARMUP_EPOCHS:\n",
        "        now_lr = LR * epoch / WARMUP_EPOCHS\n",
        "        for g in optimizer.param_groups:\n",
        "            g[\"lr\"] = now_lr\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    train_loss = total_loss / len(X_train_np)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss_sum = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            val_loss_sum += loss.item() * xb.size(0)\n",
        "\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "\n",
        "    val_loss = val_loss_sum / len(X_val_np)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch:03d} | Train {train_loss:.4f} | Val {val_loss:.4f} | Acc {val_acc:.4f}\")\n",
        "\n",
        "    if val_loss < best_loss - 1e-4:\n",
        "        best_loss = val_loss\n",
        "        best_state = model.state_dict()\n",
        "        pat = 0\n",
        "    else:\n",
        "        pat += 1\n",
        "        if pat >= PATIENCE:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(best_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1466c939"
      },
      "source": [
        "### 12. TEST ACCURACY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cad9e92"
      },
      "source": [
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        logits = model(xb)\n",
        "        preds = logits.argmax(1).cpu().numpy()\n",
        "\n",
        "        y_true.append(yb.numpy())\n",
        "        y_pred.append(preds)\n",
        "\n",
        "y_true = np.concatenate(y_true)\n",
        "y_pred = np.concatenate(y_pred)\n",
        "\n",
        "print(\"\\nFINAL TEST ACCURACY:\", accuracy_score(y_true, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c11a1c02"
      },
      "source": [
        "### 13. PLOTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "496ad6fd"
      },
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Val Loss\")\n",
        "plt.title(\"Loss Curve\"); plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(val_accs, label=\"Val Accuracy\")\n",
        "plt.title(\"Validation Accuracy\"); plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98e9a7e3"
      },
      "source": [
        "### 14. Train and Evaluate Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b64b9403"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "# Using 'saga' solver for multiclass with L1 penalty, which can be faster for large datasets\n",
        "# max_iter is increased to ensure convergence, class_weight='balanced' addresses class imbalance\n",
        "log_reg = LogisticRegression(\n",
        "    solver='saga',\n",
        "    multi_class='multinomial',\n",
        "    max_iter=1000,\n",
        "    random_state=RANDOM_SEED,\n",
        "    n_jobs=-1, # Use all available CPU cores\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "print(\"Training Logistic Regression model...\")\n",
        "log_reg.fit(X_train_np, y_train_np)\n",
        "print(\"Logistic Regression training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_log_reg = log_reg.predict(X_test_np)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\nLOGISTIC REGRESSION PERFORMANCE:\")\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test_np, y_pred_log_reg))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_np, y_pred_log_reg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd358d5b"
      },
      "source": [
        "### 15. Train and Evaluate Support Vector Machine (SVM) Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45a783d7"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# For large datasets, LinearSVC is generally preferred over SVC with a linear kernel\n",
        "# as it is implemented in terms of liblinear and is more scalable.\n",
        "# Setting dual=False when n_samples > n_features is generally recommended.\n",
        "# max_iter is increased to ensure convergence, class_weight='balanced' addresses class imbalance\n",
        "svm_model = LinearSVC(\n",
        "    random_state=RANDOM_SEED,\n",
        "    dual=False, # Recommended when n_samples > n_features (which is the case here)\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "print(\"\\nTraining LinearSVC model...\")\n",
        "svm_model.fit(X_train_np, y_train_np)\n",
        "print(\"LinearSVC training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_svm = svm_model.predict(X_test_np)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\nSVM (LinearSVC) PERFORMANCE:\")\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test_np, y_pred_svm))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_np, y_pred_svm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s5FYV0CleUDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7BKX-OIrSBhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "805e0d25"
      },
      "source": [
        "### 16. Confusion Matrix for Deep MLP Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd31a95c"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Confusion Matrix for Deep MLP\n",
        "cm_nn = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Deep MLP')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edce44fc"
      },
      "source": [
        "### 17. Confusion Matrix for Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41c33932"
      },
      "source": [
        "# Confusion Matrix for Logistic Regression\n",
        "cm_lr = confusion_matrix(y_test_np, y_pred_log_reg)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Logistic Regression')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49845457"
      },
      "source": [
        "### 18. Confusion Matrix for SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a853395c"
      },
      "source": [
        "# Confusion Matrix for SVM (LinearSVC)\n",
        "cm_svm = confusion_matrix(y_test_np, y_pred_svm)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - SVM (LinearSVC)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}